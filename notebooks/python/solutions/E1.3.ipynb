{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1.3 Solution\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# Perform dataset normalisation and configure our dataset\n",
    "def normalize(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "num_train_examples = 50000\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Create and fit our model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_dataset, \n",
    "          shuffle=True, \n",
    "          epochs=2, \n",
    "          steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    "\n",
    "# Evaluate the model with the test dataset\n",
    "model.evaluate(test_dataset)\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}